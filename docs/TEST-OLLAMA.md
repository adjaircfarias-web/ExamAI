# ‚úÖ Testes de Integra√ß√£o - Ollama

## üéØ Objetivo

Verificar se a integra√ß√£o entre ExamAI e Ollama est√° funcionando corretamente.

---

## üìã Pr√©-requisitos

Antes de testar, certifique-se:

- [ ] Ollama instalado
- [ ] Modelo `llama3.1:8b` baixado
- [ ] Ollama rodando em `http://localhost:11434`
- [ ] API ExamAI compilada sem erros

---

## üß™ Teste 1: Verificar se Ollama est√° rodando

```bash
# Teste direto na API do Ollama
curl http://localhost:11434/api/tags
```

**Response esperada:**
```json
{
  "models": [
    {
      "name": "llama3.1:8b",
      "size": 4661224448,
      "digest": "...",
      "modified_at": "2026-02-03T01:00:00Z"
    }
  ]
}
```

**Se falhar:**
- Windows: Abra o aplicativo Ollama pelo menu Iniciar
- Linux/Mac: Execute `ollama serve`

---

## üß™ Teste 2: Teste Interativo do Modelo

```bash
ollama run llama3.1:8b

>>> Ol√°, voc√™ fala portugu√™s?
# Deve responder em portugu√™s

>>> Liste 3 tipos de exames m√©dicos
# Deve listar exames

>>> /bye
```

**‚úÖ Passou:** Ollama est√° funcionando e responde em portugu√™s

---

## üß™ Teste 3: Health Check da API

### **Passo 1: Iniciar a API**

```bash
cd C:\dev\myprojects\ExamAI
dotnet run --project src/ExamAI.Api
```

**Output esperado:**
```
info: Microsoft.Hosting.Lifetime[14]
      Now listening on: http://localhost:5000
info: Microsoft.Hosting.Lifetime[0]
      Application started. Press Ctrl+C to shut down.
```

### **Passo 2: Testar Health Check Geral**

```bash
# Em outro terminal
curl http://localhost:5000/health
```

**Response esperada:**
```json
{
  "status": "healthy",
  "timestamp": "2026-02-03T01:30:00Z",
  "version": "1.0.0"
}
```

### **Passo 3: Testar Health Check do Ollama**

```bash
curl http://localhost:5000/health/ollama
```

**Response esperada:**
```json
{
  "status": "healthy",
  "service": "Ollama",
  "url": "http://localhost:11434",
  "model": "llama3.1:8b",
  "timestamp": "2026-02-03T01:30:00Z"
}
```

**Se retornar status 503:**
```json
{
  "status": "unhealthy",
  "error": "Cannot connect to Ollama service. Is Ollama running?"
}
```

**Solu√ß√£o:** Inicie o Ollama e tente novamente

### **Passo 4: Testar Health Check do PostgreSQL**

```bash
curl http://localhost:5000/health/database
```

**Se PostgreSQL n√£o estiver rodando:**
```json
{
  "status": "unhealthy",
  "error": "Cannot connect to database"
}
```

**Solu√ß√£o:** Suba o PostgreSQL (ver `docs/SETUP-POSTGRES.md`)

---

## üß™ Teste 4: Teste via Swagger

### **Passo 1: Acessar Swagger**

1. Inicie a API: `dotnet run --project src/ExamAI.Api`
2. Abra o navegador: `http://localhost:5000/swagger`

### **Passo 2: Testar endpoints na UI**

1. Expanda `/health/ollama`
2. Clique em "Try it out"
3. Clique em "Execute"
4. Verifique a resposta

---

## üß™ Teste 5: Teste de Infer√™ncia B√°sica (Manual)

Este teste ser√° usado quando implementarmos os Agents.

**Exemplo de c√≥digo (para refer√™ncia futura):**

```csharp
// Isso ser√° implementado no ExtractionAgent
var messages = new List<ChatMessage>
{
    new(ChatRole.System, "Voc√™ √© um assistente que extrai dados de exames m√©dicos."),
    new(ChatRole.User, "Extraia os dados: Colesterol Total: 210 mg/dL")
};

var response = await chatClient.CompleteAsync(messages);
Console.WriteLine(response.Message.Text);
```

---

## üìä Checklist de Valida√ß√£o

Marque conforme completa:

### **Setup**
- [ ] Ollama instalado e rodando
- [ ] Modelo llama3.1:8b baixado
- [ ] API ExamAI compila sem erros

### **Testes Ollama**
- [ ] `curl http://localhost:11434/api/tags` retorna 200
- [ ] `ollama run llama3.1:8b` funciona
- [ ] Modelo responde em portugu√™s

### **Testes API**
- [ ] `GET /health` retorna 200
- [ ] `GET /health/ollama` retorna 200 (healthy)
- [ ] `GET /health/database` retorna 200 ou 503 (ok se PostgreSQL n√£o estiver rodando ainda)

### **Logs**
- [ ] Logs mostram "Ollama client configured successfully"
- [ ] Sem erros de conex√£o nos logs

---

## üêõ Troubleshooting

### Problema: "Cannot connect to Ollama service"

**Diagn√≥stico:**
```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags
```

**Solu√ß√µes:**
1. Iniciar Ollama: Abra o app Ollama ou execute `ollama serve`
2. Verificar porta: Certifique-se que 11434 n√£o est√° bloqueada
3. Firewall: Permita conex√µes locais na porta 11434

### Problema: "Model not found"

**Diagn√≥stico:**
```bash
ollama list
```

**Solu√ß√£o:**
```bash
ollama pull llama3.1:8b
```

### Problema: API n√£o inicia

**Diagn√≥stico:**
```bash
cd C:\dev\myprojects\ExamAI
dotnet build
```

**Solu√ß√µes:**
1. Verificar erros de compila√ß√£o
2. Limpar e rebuildar: `dotnet clean && dotnet build`
3. Verificar refer√™ncias entre projetos

---

## ‚úÖ Resultado Esperado

Ap√≥s completar todos os testes:

```
‚úÖ Ollama rodando
‚úÖ Modelo llama3.1:8b dispon√≠vel
‚úÖ API compila sem erros
‚úÖ Health checks retornam 200
‚úÖ IChatClient configurado corretamente
```

---

## üöÄ Pr√≥ximos Passos

Com Ollama validado, voc√™ pode:

1. ‚úÖ Implementar parsers de documentos (US-005, US-006, US-007)
2. ‚úÖ Implementar ExtractionAgent com Ollama (US-009)
3. ‚úÖ Processar seu primeiro exame m√©dico!

---

**√öltima atualiza√ß√£o:** 03/02/2026  
**Status:** Pronto para testes
