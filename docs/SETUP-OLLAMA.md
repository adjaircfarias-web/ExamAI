# ü§ñ Setup Ollama para ExamAI

## O que √© Ollama?

Ollama permite rodar **Large Language Models (LLMs) localmente** no seu computador, sem enviar dados para a nuvem. Ideal para privacidade e custo zero por infer√™ncia.

---

## ‚úÖ Pr√©-requisitos

- Windows 10/11, Linux ou macOS
- ~8GB RAM dispon√≠vel
- ~6GB espa√ßo em disco (para o modelo llama3.1:8b)
- GPU NVIDIA (opcional, mas recomendado para melhor performance)

---

## üì¶ Instala√ß√£o

### **Windows**

```powershell
# Op√ß√£o 1: Instalador oficial
# Baixar de: https://ollama.com/download/windows

# Op√ß√£o 2: Winget
winget install Ollama.Ollama

# Verificar instala√ß√£o
ollama --version
```

### **Linux**

```bash
curl -fsSL https://ollama.com/install.sh | sh

# Verificar
ollama --version
```

### **macOS**

```bash
# Baixar de: https://ollama.com/download/mac
# Ou via Homebrew
brew install ollama

# Verificar
ollama --version
```

---

## üöÄ Baixar o Modelo

```bash
# Baixar Llama 3.1 8B (modelo recomendado)
ollama pull llama3.1:8b

# Verificar modelos instalados
ollama list
```

**Output esperado:**
```
NAME                ID              SIZE    MODIFIED
llama3.1:8b         abc123def456    4.7 GB  2 minutes ago
```

---

## ‚úÖ Testar o Ollama

### **1. Teste Interativo**

```bash
ollama run llama3.1:8b

>>> Ol√°, voc√™ entende portugu√™s?
Sim, eu entendo portugu√™s! Como posso ajud√°-lo?

>>> /bye
```

### **2. Testar API REST**

```bash
# Verificar se a API est√° rodando
curl http://localhost:11434/api/tags

# Testar gera√ß√£o de texto
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "Por que o c√©u √© azul?",
  "stream": false
}'
```

---

## üîå Integra√ß√£o com ExamAI

### **1. Configura√ß√£o no appsettings.json**

J√° est√° configurado! Veja em `src/ExamAI.Api/appsettings.json`:

```json
{
  "Ollama": {
    "Url": "http://localhost:11434",
    "Model": "llama3.1:8b",
    "Temperature": 0.1,
    "MaxTokens": 4096,
    "TimeoutSeconds": 60
  }
}
```

### **2. Testar Health Check**

```bash
# Certifique-se que a API est√° rodando
cd C:\dev\myprojects\ExamAI
dotnet run --project src/ExamAI.Api

# Em outro terminal, teste o health check
curl http://localhost:5000/health/ollama
```

**Response esperada:**
```json
{
  "status": "healthy",
  "model": "llama3.1:8b",
  "responseTime": "2026-02-03T01:30:00Z",
  "response": "pong"
}
```

---

## üêõ Troubleshooting

### **Erro: "connection refused"**

**Causa:** Ollama n√£o est√° rodando

**Solu√ß√£o:**
```bash
# Windows: Ollama inicia automaticamente, mas se n√£o estiver:
# Abra o aplicativo Ollama pelo menu Iniciar

# Linux/Mac: Iniciar o servi√ßo
ollama serve
```

### **Erro: "model not found"**

**Causa:** Modelo n√£o foi baixado

**Solu√ß√£o:**
```bash
ollama pull llama3.1:8b
```

### **Erro: "Out of memory" ou muito lento**

**Causa:** RAM insuficiente ou usando CPU em vez de GPU

**Solu√ß√£o:**

**1. Usar modelo menor:**
```bash
ollama pull llama3.1:3b  # Modelo menor, mais r√°pido
```

**2. Verificar se GPU est√° sendo usada:**
```bash
# NVIDIA
nvidia-smi

# Durante infer√™ncia, deve mostrar uso da GPU
```

**3. For√ßar CPU-only (se GPU n√£o funcionar):**
```bash
# Windows
$env:OLLAMA_NUM_GPU=0
ollama serve

# Linux/Mac
OLLAMA_NUM_GPU=0 ollama serve
```

### **Erro: "failed to allocate memory"**

**Causa:** Modelo muito grande para GPU dispon√≠vel

**Solu√ß√£o:**
```bash
# Usar modelo quantizado (menor)
ollama pull llama3.1:8b-q4_0  # Vers√£o quantizada

# Ou modelo menor
ollama pull llama3.1:3b
```

---

## üìä Comparativo de Modelos

| Modelo | Tamanho | VRAM | RAM | Velocidade | Qualidade |
|--------|---------|------|-----|------------|-----------|
| **llama3.1:3b** | 2 GB | ~3 GB | ~4 GB | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |
| **llama3.1:8b** | 4.7 GB | ~6 GB | ~8 GB | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **llama3.1:70b** | 40 GB | ~42 GB | ~64 GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**Recomenda√ß√£o para ExamAI:** `llama3.1:8b` (melhor equil√≠brio)

---

## üîß Configura√ß√µes Avan√ßadas

### **Alterar Modelo no ExamAI**

Edite `appsettings.json`:

```json
{
  "Ollama": {
    "Model": "llama3.1:3b"  // Trocar para modelo menor
  }
}
```

### **Ajustar Temperature**

- **Temperature = 0.0:** Mais determin√≠stico (sempre mesma resposta)
- **Temperature = 0.1:** Levemente vari√°vel (recomendado para extra√ß√£o)
- **Temperature = 1.0:** Mais criativo

```json
{
  "Ollama": {
    "Temperature": 0.1  // Recomendado para dados estruturados
  }
}
```

### **Ver Logs do Ollama**

```bash
# Windows
%USERPROFILE%\.ollama\logs\server.log

# Linux/Mac
~/.ollama/logs/server.log

# Ver logs em tempo real
tail -f ~/.ollama/logs/server.log
```

---

## üîê Seguran√ßa

### **Ollama √© Seguro?**

‚úÖ **Sim!** Dados processados 100% localmente
- Nenhum dado enviado para a nuvem
- Sem telemetria
- C√≥digo open-source

### **LGPD Compliance**

‚úÖ **Totalmente compat√≠vel:**
- Dados m√©dicos nunca saem do servidor
- Sem transfer√™ncia internacional de dados
- Sem processamento por terceiros

---

## üìö Recursos √öteis

- **Documenta√ß√£o Oficial:** https://ollama.com/docs
- **Modelos Dispon√≠veis:** https://ollama.com/library
- **GitHub:** https://github.com/ollama/ollama
- **Discord:** https://discord.gg/ollama

---

## ‚úÖ Checklist de Verifica√ß√£o

Antes de continuar, verifique:

- [ ] Ollama instalado e rodando
- [ ] Modelo `llama3.1:8b` baixado
- [ ] API REST respondendo em `http://localhost:11434`
- [ ] Health check `/health/ollama` retorna 200 OK
- [ ] Teste interativo funciona (`ollama run llama3.1:8b`)

---

## üöÄ Pr√≥ximos Passos

Com Ollama configurado, voc√™ pode:

1. ‚úÖ Testar extra√ß√£o de texto de documentos
2. ‚úÖ Implementar os Agents (ExtractionAgent, ValidationAgent, etc.)
3. ‚úÖ Processar seus primeiros exames m√©dicos

---

**√öltima atualiza√ß√£o:** 03/02/2026  
**Vers√£o Ollama recomendada:** 0.x.x  
**Modelo recomendado:** llama3.1:8b
