# ğŸ¤– Multi-Agent Framework (MAF) com LLM Local - RecomendaÃ§Ã£o

**Data:** 29/01/2026  
**Autor:** Clawdex ğŸ”  
**Contexto:** ExtraÃ§Ã£o de dados de exames clÃ­nicos usando arquitetura MAF com Ollama

---

## ğŸ¯ RecomendaÃ§Ã£o: Arquitetura MAF com LLM Local

### **Stack Sugerida**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Orquestrador Central (MAF)             â”‚
â”‚  (Gerencia workflow e comunicaÃ§Ã£o entre agents) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚         â”‚            â”‚            â”‚          â”‚
   â–¼         â–¼            â–¼            â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”
â”‚Agentâ”‚  â”‚Agentâ”‚      â”‚Agentâ”‚     â”‚Agentâ”‚    â”‚Agentâ”‚
â”‚Doc  â”‚  â”‚OCR  â”‚      â”‚Extr.â”‚     â”‚Validâ”‚    â”‚Norm.â”‚
â”‚Parseâ”‚  â”‚     â”‚      â”‚     â”‚     â”‚     â”‚    â”‚     â”‚
â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜
   â”‚         â”‚            â”‚            â”‚          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                    â”‚  Ollama â”‚
                    â”‚ (Local) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ ImplementaÃ§Ã£o PrÃ¡tica

### **1. LLM Local - Ollama**

```bash
# Instalar Ollama
# Windows: baixar de https://ollama.com
# Linux/Mac: curl -fsSL https://ollama.com/install.sh | sh

# Baixar modelo recomendado para extraÃ§Ã£o mÃ©dica
ollama pull llama3.1:8b           # RÃ¡pido, bom custo-benefÃ­cio
ollama pull qwen2.5:14b           # Melhor em portuguÃªs + JSON
ollama pull mistral:7b-instruct   # Alternativa veloz

# Para melhor qualidade (precisa de GPU boa)
ollama pull llama3.1:70b
```

---

### **2. Definir os Agentes Especializados**

Cada agente tem uma responsabilidade Ãºnica:

```csharp
// Agent 1: Document Parser
public class DocumentParserAgent
{
    // Recebe: Stream do arquivo
    // Retorna: Texto bruto extraÃ­do
    public Task<string> ParseAsync(Stream file, string type);
}

// Agent 2: OCR Agent (para imagens/PDFs escaneados)
public class OcrAgent
{
    // Recebe: Imagem/PDF escaneado
    // Retorna: Texto extraÃ­do via Tesseract ou Azure Vision
    public Task<string> ExtractTextFromImageAsync(Stream image);
}

// Agent 3: Extraction Agent (LLM local)
public class ExtractionAgent
{
    private readonly IChatClient _llm; // Ollama

    // Recebe: Texto bruto
    // Retorna: JSON estruturado com exames
    public Task<ExamResult> ExtractStructuredDataAsync(string text);
}

// Agent 4: Validation Agent
public class ValidationAgent
{
    // Recebe: JSON extraÃ­do
    // Retorna: JSON validado + warnings
    public Task<ValidationResult> ValidateAsync(ExamResult exam);
}

// Agent 5: Normalization Agent
public class NormalizationAgent
{
    // Recebe: JSON validado
    // Retorna: JSON normalizado (unidades, nomes, etc.)
    public Task<ExamResult> NormalizeAsync(ExamResult exam);
}
```

---

### **3. Orquestrador (MAF Controller)**

```csharp
public class MedicalExamPipeline
{
    private readonly DocumentParserAgent _parser;
    private readonly OcrAgent _ocr;
    private readonly ExtractionAgent _extractor;
    private readonly ValidationAgent _validator;
    private readonly NormalizationAgent _normalizer;
    private readonly ILogger<MedicalExamPipeline> _logger;

    public async Task<ExamResult> ProcessExamAsync(
        Stream fileStream,
        string fileType)
    {
        _logger.LogInformation("Starting exam processing pipeline");

        // Step 1: Parse documento
        var rawText = fileType is ".jpg" or ".png"
            ? await _ocr.ExtractTextFromImageAsync(fileStream)
            : await _parser.ParseAsync(fileStream, fileType);

        _logger.LogInformation("Document parsed: {Length} chars", rawText.Length);

        // Step 2: Extrair dados estruturados (LLM local)
        var extractedData = await _extractor.ExtractStructuredDataAsync(rawText);
        _logger.LogInformation("Extracted {Count} exams", extractedData.Exames.Count);

        // Step 3: Validar
        var validation = await _validator.ValidateAsync(extractedData);
        if (!validation.IsValid)
        {
            _logger.LogWarning("Validation issues: {Issues}", 
                string.Join(", ", validation.Warnings));
        }

        // Step 4: Normalizar
        var normalized = await _normalizer.NormalizeAsync(extractedData);
        _logger.LogInformation("Pipeline completed successfully");

        return normalized;
    }
}
```

---

### **4. Configurar Ollama no .NET**

```csharp
// Program.cs
builder.Services.AddSingleton<IChatClient>(sp =>
{
    var config = sp.GetRequiredService<IConfiguration>();
    var ollamaUrl = config["Ollama:Url"] ?? "http://localhost:11434";
    var model = config["Ollama:Model"] ?? "qwen2.5:14b";
    
    return new OllamaChatClient(ollamaUrl, model)
        .AsBuilder()
        .UseLogging(sp.GetRequiredService<ILoggerFactory>())
        .Build();
});

// Agents
builder.Services.AddScoped<DocumentParserAgent>();
builder.Services.AddScoped<OcrAgent>();
builder.Services.AddScoped<ExtractionAgent>();
builder.Services.AddScoped<ValidationAgent>();
builder.Services.AddScoped<NormalizationAgent>();
builder.Services.AddScoped<MedicalExamPipeline>();

// appsettings.json
{
  "Ollama": {
    "Url": "http://localhost:11434",
    "Model": "qwen2.5:14b"
  }
}
```

---

### **5. Extraction Agent com Ollama**

```csharp
public class ExtractionAgent
{
    private readonly IChatClient _llm;
    private readonly ILogger<ExtractionAgent> _logger;

    public ExtractionAgent(IChatClient llm, ILogger<ExtractionAgent> logger)
    {
        _llm = llm;
        _logger = logger;
    }

    public async Task<ExamResult> ExtractStructuredDataAsync(string documentText)
    {
        var systemPrompt = @"
VocÃª Ã© um especialista em anÃ¡lise de exames clÃ­nicos brasileiros.
Extraia TODOS os resultados de exames em formato JSON.

REGRAS:
- Normalize nomes: 'Col. Total' â†’ 'Colesterol Total'
- Extraia valores numÃ©ricos + unidades
- Identifique valores de referÃªncia
- Classifique: normal, baixo, alto, crÃ­tico
- SEMPRE retorne JSON vÃ¡lido (sem markdown)
- Se nÃ£o encontrar dados, retorne arrays vazios

FORMATO DE SAÃDA:
{
  ""paciente"": {
    ""nome"": ""string ou null"",
    ""data_nascimento"": ""yyyy-MM-dd ou null"",
    ""data_coleta"": ""yyyy-MM-dd ou null"",
    ""medico_solicitante"": ""string ou null""
  },
  ""exames"": [
    {
      ""tipo"": ""Colesterol Total"",
      ""valor"": 200,
      ""unidade"": ""mg/dL"",
      ""referencia_min"": 0,
      ""referencia_max"": 200,
      ""status"": ""normal"",
      ""observacoes"": null
    }
  ]
}
";

        var userPrompt = $@"
Analise este laudo mÃ©dico:

=== INÃCIO ===
{documentText}
=== FIM ===

Retorne JSON puro (sem ```json):
";

        var messages = new List<ChatMessage>
        {
            new(ChatRole.System, systemPrompt),
            new(ChatRole.User, userPrompt)
        };

        var options = new ChatOptions
        {
            Temperature = 0.1f,  // Baixa temperatura = mais determinÃ­stico
            MaxTokens = 4000
        };

        _logger.LogInformation("Calling Ollama for extraction");
        var response = await _llm.CompleteAsync(messages, options);
        
        var jsonText = CleanJsonResponse(response.Message.Text);
        _logger.LogDebug("LLM Response: {Json}", jsonText);

        try
        {
            return JsonSerializer.Deserialize<ExamResult>(jsonText,
                new JsonSerializerOptions { PropertyNameCaseInsensitive = true })
                ?? throw new InvalidOperationException("Deserialization returned null");
        }
        catch (JsonException ex)
        {
            _logger.LogError(ex, "Failed to parse JSON from LLM: {Json}", jsonText);
            throw new InvalidOperationException("LLM returned invalid JSON", ex);
        }
    }

    private static string CleanJsonResponse(string response)
    {
        // Remove markdown code blocks se presentes
        response = response.Trim();
        if (response.StartsWith("```json"))
            response = response["```json".Length..];
        if (response.StartsWith("```"))
            response = response[3..];
        if (response.EndsWith("```"))
            response = response[..^3];
        
        return response.Trim();
    }
}
```

---

## ğŸš€ Comparativo: LLM Local vs Cloud

| Aspecto | **Ollama (Local)** | **OpenAI GPT-4** |
|---------|-------------------|------------------|
| **Custo** | ğŸ’° GrÃ¡tis (energia + hardware) | ğŸ’°ğŸ’°ğŸ’° $0.05-0.20/doc |
| **Privacidade** | ğŸ”’ 100% local | âš ï¸ Dados vÃ£o pra cloud |
| **Velocidade** | âš¡ 5-20s (depende da GPU) | âš¡ 3-10s |
| **PrecisÃ£o** | ğŸ¯ 80-90% (Llama3.1 70B) | ğŸ¯ 90-98% |
| **Offline** | âœ… Sim | âŒ NÃ£o |
| **Setup** | ğŸ”§ MÃ©dio (instalar Ollama) | ğŸ”§ FÃ¡cil (sÃ³ API key) |

---

## ğŸ’¡ RecomendaÃ§Ãµes Finais

### **Para ProduÃ§Ã£o com Privacidade:**
```
âœ… Use Ollama + Qwen2.5:14b (bom equilÃ­brio portuguÃªs/precisÃ£o)
âœ… Implemente fallback para GPT-4 se LLM local falhar
âœ… Use GPU decente (RTX 3060+ ou A4000+)
âœ… Cache resultados (mesmo documento = mesma resposta)
```

### **Para Prototipagem RÃ¡pida:**
```
âœ… Use GPT-4 (menos config, maior precisÃ£o)
âœ… Migre pra local depois de validar o fluxo
âœ… Mantenha a abstraÃ§Ã£o IChatClient (troca fÃ¡cil)
```

### **Modelos Locais Recomendados:**

1. **qwen2.5:14b** â† Melhor para portuguÃªs + JSON estruturado
2. **llama3.1:8b** â† Mais rÃ¡pido, boa qualidade
3. **mistral:7b-instruct** â† Alternativa rÃ¡pida
4. **llama3.1:70b** â† MÃ¡xima qualidade (precisa GPU potente)

---

## ğŸ“¦ Setup Completo

```bash
# 1. Instalar Ollama
winget install Ollama.Ollama

# 2. Baixar modelo
ollama pull qwen2.5:14b

# 3. Testar
ollama run qwen2.5:14b
>>> OlÃ¡, vocÃª entende portuguÃªs?
```

```bash
# 4. No projeto .NET
dotnet add package Microsoft.Extensions.AI.Ollama --prerelease
dotnet add package itext7
dotnet add package DocumentFormat.OpenXml
dotnet add package EPPlus
```

---

## ğŸ—ï¸ Arquitetura Completa do Sistema

### **Pipeline de Processamento**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Upload de Doc   â”‚
â”‚  (PDF/Word/Excel)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ DocumentParserAgent â”‚
    â”‚ (iText7/OpenXml)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚ Texto Raw â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ExtractionAgent     â”‚
    â”‚  (Ollama/Qwen2.5)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ JSON Estruturado  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ValidationAgent   â”‚
    â”‚ (Regras negÃ³cio)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ NormalizationAgentâ”‚
    â”‚ (PadronizaÃ§Ã£o)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ExamResult      â”‚
    â”‚   (Finalizado)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Performance Esperada

### **MÃ©tricas com Qwen2.5:14b**

| MÃ©trica | Valor Esperado |
|---------|----------------|
| **PrecisÃ£o** | 85-92% |
| **Tempo/Documento** | 8-15 segundos |
| **Consumo GPU** | 6-8GB VRAM |
| **Custo** | $0 (sÃ³ energia) |
| **Taxa de Sucesso** | 90%+ |

### **Hardware Recomendado**

- **MÃ­nimo:** RTX 3060 (12GB), 16GB RAM
- **Recomendado:** RTX 4070 (12GB), 32GB RAM
- **Ideal:** RTX 4090 (24GB), 64GB RAM ou A100/H100

---

## ğŸ§ª Exemplo de Uso

### **1. Controller Endpoint**

```csharp
[ApiController]
[Route("api/[controller]")]
public class ExamsController : ControllerBase
{
    private readonly MedicalExamPipeline _pipeline;

    [HttpPost("upload")]
    public async Task<IActionResult> UploadExam(IFormFile file)
    {
        using var stream = file.OpenReadStream();
        var extension = Path.GetExtension(file.FileName);
        
        var result = await _pipeline.ProcessExamAsync(stream, extension);
        
        return Ok(result);
    }
}
```

### **2. Request Example**

```bash
curl -X POST http://localhost:5000/api/exams/upload \
  -F "file=@exame_colesterol.pdf"
```

### **3. Response Example**

```json
{
  "paciente": {
    "nome": "JoÃ£o Silva",
    "dataNascimento": "1980-05-15",
    "dataColeta": "2026-01-28",
    "medicoSolicitante": "Dra. Maria Santos"
  },
  "exames": [
    {
      "tipo": "Colesterol Total",
      "valor": 210,
      "unidade": "mg/dL",
      "referenciaMin": 0,
      "referenciaMax": 200,
      "status": "alto",
      "observacoes": null
    },
    {
      "tipo": "HDL",
      "valor": 45,
      "unidade": "mg/dL",
      "referenciaMin": 40,
      "referenciaMax": 60,
      "status": "normal",
      "observacoes": null
    }
  ]
}
```

---

## ğŸ”„ Fallback Strategy (HÃ­brido Local + Cloud)

Para mÃ¡xima confiabilidade, implemente fallback:

```csharp
public class HybridExtractionAgent
{
    private readonly IChatClient _localLlm;    // Ollama
    private readonly IChatClient _cloudLlm;    // GPT-4
    private readonly ILogger _logger;

    public async Task<ExamResult> ExtractAsync(string text)
    {
        try
        {
            _logger.LogInformation("Trying local LLM first");
            return await ExtractWithLlmAsync(_localLlm, text);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Local LLM failed, falling back to cloud");
            return await ExtractWithLlmAsync(_cloudLlm, text);
        }
    }

    private async Task<ExamResult> ExtractWithLlmAsync(
        IChatClient llm, 
        string text)
    {
        // LÃ³gica de extraÃ§Ã£o compartilhada
        var response = await llm.CompleteAsync(BuildPrompt(text));
        return ParseResponse(response);
    }
}
```

---

## ğŸ¯ Casos de Uso AvanÃ§ados

### **1. Batch Processing**

```csharp
public async Task<List<ExamResult>> ProcessBatchAsync(
    IEnumerable<Stream> files)
{
    var tasks = files.Select(f => 
        _pipeline.ProcessExamAsync(f, ".pdf")
    );
    
    return (await Task.WhenAll(tasks)).ToList();
}
```

### **2. AnÃ¡lise Comparativa (Temporal)**

```csharp
public class TemporalAnalyzer
{
    public async Task<TrendAnalysis> AnalyzeTrendAsync(
        List<ExamResult> historicalExams)
    {
        // Usar LLM para analisar tendÃªncias
        var prompt = $@"
            Analise esta sÃ©rie temporal de exames:
            {JsonSerializer.Serialize(historicalExams)}
            
            Identifique:
            - TendÃªncias (subindo/descendo)
            - Valores anÃ´malos
            - PadrÃµes preocupantes
        ";
        
        var analysis = await _llm.CompleteAsync(prompt);
        return ParseTrendAnalysis(analysis);
    }
}
```

### **3. GeraÃ§Ã£o de RelatÃ³rios**

```csharp
public async Task<string> GenerateReportAsync(ExamResult exam)
{
    var prompt = $@"
        Gere um relatÃ³rio em portuguÃªs simples baseado nestes resultados:
        {JsonSerializer.Serialize(exam)}
        
        O relatÃ³rio deve:
        - Destacar valores anormais
        - Sugerir acompanhamento mÃ©dico se necessÃ¡rio
        - Usar linguagem acessÃ­vel (sem jargÃ£o)
        - MÃ¡ximo 200 palavras
    ";
    
    var response = await _llm.CompleteAsync(prompt);
    return response.Message.Text;
}
```

---

## âš ï¸ LimitaÃ§Ãµes e ConsideraÃ§Ãµes

### **LimitaÃ§Ãµes do LLM Local**

1. **PrecisÃ£o:** 5-10% menor que GPT-4 em casos complexos
2. **Hardware:** Requer GPU decente (custo inicial)
3. **LatÃªncia:** Pode ser mais lento que APIs cloud otimizadas
4. **Contexto:** Janela de contexto menor (4k-32k vs 128k do GPT-4)

### **Quando NÃƒO usar LLM Local**

- âŒ Documentos complexos demais (manuscritos ilegÃ­veis)
- âŒ PrecisÃ£o crÃ­tica (decisÃµes mÃ©dicas automatizadas)
- âŒ Hardware limitado (sem GPU ou <8GB VRAM)
- âŒ Volume muito alto (cloud pode ser mais eficiente)

### **Quando SIM usar LLM Local**

- âœ… Dados sensÃ­veis (LGPD/HIPAA compliance)
- âœ… Infraestrutura prÃ³pria (data centers)
- âœ… Volume previsÃ­vel (custo fixo)
- âœ… Baixa latÃªncia de rede (edge computing)

---

## ğŸš€ Roadmap de ImplementaÃ§Ã£o

### **Fase 1 - MVP (2 semanas)**
- [ ] Setup Ollama + Qwen2.5:14b
- [ ] Implementar DocumentParserAgent
- [ ] Implementar ExtractionAgent (bÃ¡sico)
- [ ] API REST simples
- [ ] Testes com 10 documentos reais

### **Fase 2 - ValidaÃ§Ã£o (1 semana)**
- [ ] Implementar ValidationAgent
- [ ] Implementar NormalizationAgent
- [ ] Testes de precisÃ£o (50+ documentos)
- [ ] Benchmarking vs GPT-4

### **Fase 3 - ProduÃ§Ã£o (2 semanas)**
- [ ] Fallback cloud (hÃ­brido)
- [ ] Cache de resultados
- [ ] Observabilidade (logs, mÃ©tricas)
- [ ] Deploy em ambiente de produÃ§Ã£o
- [ ] DocumentaÃ§Ã£o completa

### **Fase 4 - Melhorias (contÃ­nuo)**
- [ ] Fine-tuning do modelo local (se necessÃ¡rio)
- [ ] Dashboard de anÃ¡lise
- [ ] IntegraÃ§Ã£o com sistemas externos
- [ ] Machine Learning para prÃ©-classificaÃ§Ã£o

---

## ğŸ“š Recursos Ãšteis

### **DocumentaÃ§Ã£o**
- [Ollama Docs](https://ollama.ai/docs)
- [Microsoft.Extensions.AI](https://learn.microsoft.com/dotnet/ai/)
- [Qwen2.5 Model Card](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)

### **Ferramentas**
- [Ollama Web UI](https://github.com/open-webui/open-webui) - Interface grÃ¡fica
- [LM Studio](https://lmstudio.ai/) - Alternativa ao Ollama
- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) - OCR local gratuito

### **Comunidades**
- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA)
- [Ollama Discord](https://discord.gg/ollama)
- [.NET AI Community](https://discord.gg/dotnet)

---

## âœ… ConclusÃ£o

Para o caso de uso de **extraÃ§Ã£o de exames clÃ­nicos**, a arquitetura MAF com **Ollama + Qwen2.5:14b** oferece:

### **Vantagens:**
- âœ… **Privacidade total** (dados nÃ£o saem do servidor)
- âœ… **Custo zero** por documento processado
- âœ… **Controle total** sobre o modelo e pipeline
- âœ… **Offline-first** (nÃ£o depende de internet)
- âœ… **EscalÃ¡vel** (adicionar GPUs conforme necessÃ¡rio)

### **Trade-offs:**
- âš ï¸ **PrecisÃ£o 5-10% menor** que GPT-4
- âš ï¸ **Setup inicial mais complexo**
- âš ï¸ **Hardware dedicado necessÃ¡rio**
- âš ï¸ **ManutenÃ§Ã£o** (updates de modelos)

### **RecomendaÃ§Ã£o Final:**

**Use abordagem hÃ­brida:**
1. **Ollama como primÃ¡rio** (custo zero, privacidade)
2. **GPT-4 como fallback** (casos complexos/falhas)
3. **Monitorar precisÃ£o** (comparar resultados periodicamente)
4. **Iterar e melhorar** (ajustar prompts, testar modelos novos)

---

**Ãšltima atualizaÃ§Ã£o:** 29/01/2026  
**VersÃ£o:** 1.0  
**Status:** Pronto para implementaÃ§Ã£o ğŸš€
